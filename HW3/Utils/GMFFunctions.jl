@with_kw struct GaussianMixtureBelief
    components::Array{FullNormal}  # Array of Gaussian components
    weights::Array{Float64, 1}     # Component weights
end

"""
    mixtureMoments(gmb::GaussianMixtureBelief)

Returns (ŒºÃÑ, Œ£ÃÑ) ‚Äì the weighted mean and full covariance of the
Gaussian‚Äêmixture belief

    ŒºÃÑ = ‚àë w·µ¢ Œº·µ¢
    Œ£ÃÑ = ‚àë w·µ¢ [ Œ£·µ¢ + (Œº·µ¢-ŒºÃÑ)(Œº·µ¢-ŒºÃÑ)·µÄ ]
"""
function mixtureMoments(gmb::GaussianMixtureBelief)
    ŒºÃÑ = sum(gmb.weights[i] .* gmb.components[i].Œº for i in eachindex(gmb.weights))
    Œ£ÃÑ = zeros(length(ŒºÃÑ), length(ŒºÃÑ))
    for i in eachindex(gmb.weights)
        Œºi  = gmb.components[i].Œº
        Œ£i  = gmb.components[i].Œ£
        w   = gmb.weights[i]
        Œ£ÃÑ .+= w .* (Œ£i .+ (Œºi-ŒºÃÑ)*(Œºi-ŒºÃÑ)')
    end
    return ŒºÃÑ, Œ£ÃÑ
end

"""
    printMixture(gmb::GaussianMixtureBelief; step::Int)

Prints every component‚Äôs weight and mean plus the overall weighted mean.
"""
function printMixture(gmb::GaussianMixtureBelief; step::Int)
    println("‚îÄ‚îÄ GMF  step $step  ‚îÄ‚îÄ")
    for (i,(comp,w)) in enumerate(zip(gmb.components, gmb.weights))
        println("  ‚ñ∏ comp $i :  w=$(round(w,digits=4))  Œº=$(round.(comp.Œº; digits=3))")
    end
    ŒºÃÑ, Œ£ÃÑ = mixtureMoments(gmb)

    # show overall mean and the two-by-two covariance nicely formatted
    println("  Œ£w Œº  = ", round.(ŒºÃÑ; digits=3))
    println("  Œ£w Œ£ = ")
    for r in 1:size(Œ£ÃÑ,1)
        println("          ", round.(Œ£ÃÑ[r,:]; digits=3))
    end
    println()               # blank line for readability
end

# ---------- helpers for adaptive growth -------------------------------

"Mahalanobis innovation ‚Äì crude non-linearity score"
nonlinScore(comp::FullNormal, o, ùí´::POMDPscenario) =
    let H = I(2),           # identity for simple range sensors
        S = H*comp.Œ£*H' + ùí´.Œ£v,
        y = o - H*comp.Œº
    y' * inv(S) * y          # œá¬≤ distance
    end

"Split a component into two along its strongest axis"
function splitComponent(comp::FullNormal; c=0.6)
    Œª, V = eigen(comp.Œ£)
    Œ¥x   = c*sqrt(Œª[1])*V[:,1]  # major axis
    Œ¥y   = c*sqrt(Œª[2])*V[:,2]  # minor axis

    parts  = [MvNormal(comp.Œº + Œ¥x, comp.Œ£/4),
              MvNormal(comp.Œº - Œ¥x, comp.Œ£/4),
              MvNormal(comp.Œº + Œ¥y, comp.Œ£/4),
              MvNormal(comp.Œº - Œ¥y, comp.Œ£/4)]
    return parts, fill(0.25, 4)
end



function GetInitialGMBelief(Œº0, Œ£0; num_components::Int = 1, split_strategy::String = "sigma_points")
    if num_components == 1
        # Single component initialization
        component = MvNormal(Œº0, Œ£0)
        return GaussianMixtureBelief([component], [1.0])
    else
        # Multi-component initialization
        components = FullNormal[]
        weights = Float64[]
        
        if split_strategy == "grid"
            # Split based on grid positions (useful for your 9√ó9 grid)
            weight_per_component = 1.0 / num_components
            œÉ_split = sqrt(tr(Œ£0) / num_components)  # Reduce variance per component
            
            for i in 1:num_components
                # Distribute means around the original mean
                offset_x = (i % 3 - 1) * 2.0  # -2, 0, 2 grid spacing
                offset_y = (div(i-1, 3) % 3 - 1) * 2.0
                
                Œº_split = Œº0 + [offset_x, offset_y]
                Œ£_split = Œ£0 / num_components  # Smaller covariance per component
                
                push!(components, MvNormal(Œº_split, Œ£_split))
                push!(weights, weight_per_component)
            end
        elseif split_strategy == "sigma_points"
            # Use sigma-point-like initialization for better coverage
            components, weights = CreateSigmaPointGMF(Œº0, Œ£0, num_components)
        end
        
        return GaussianMixtureBelief(components, weights)
    end
end



function PropagateBelief(gmb::GaussianMixtureBelief, ùí´::POMDPscenario, a::Array{Float64, 1})
    new_components = FullNormal[]
    new_weights = Float64[]
    
    for (i, component) in enumerate(gmb.components)
        # Propagate each Gaussian component using standard Kalman prediction
        propagated = PropagateBelief(component, ùí´, a)
        push!(new_components, propagated)
        push!(new_weights, gmb.weights[i])
    end
    
    return GaussianMixtureBelief(new_components, new_weights)
end


function PropagateUpdateBelief(gmb::GaussianMixtureBelief, ùí´::POMDPscenario, a::Array{Float64, 1}, o::Array{Float64, 1})
    # First propagate
    predicted_gmb = PropagateBelief(gmb, ùí´, a)
    
    new_components = FullNormal[]
    new_weights = Float64[]
    
    for (i, component) in enumerate(predicted_gmb.components)

        # Calculate likelihood weight for this component
        likelihood_weight = obsLikelihood(ùí´, o, component.Œº)
        predicted_weight = predicted_gmb.weights[i]
        
        if likelihood_weight < 1e-10
            push!(new_components, component)  # Keep the component
            push!(new_weights, predicted_weight * 1e-10)  # Avoid zero weight
            continue
        end

        # Update each component with the observation
        updated_component = PropagateUpdateBelief(component, ùí´, zeros(2), o)
        
        updated_weight = predicted_gmb.weights[i] * likelihood_weight
        
        if updated_weight > 1e-10  # Threshold for numerical stability
            push!(new_components, updated_component)
            push!(new_weights, updated_weight)
        end
    end
    
    # Normalize weights
    if !isempty(new_weights)
        new_weights ./= sum(new_weights)
    else
        # Fallback: return single component with uniform prior
        return GetInitialGMBelief([0.0, 0.0], [4.0 0.0; 0.0 4.0])
    end
    
    return GaussianMixtureBelief(new_components, new_weights)
end


function PruneComponents(gmb::GaussianMixtureBelief; threshold::Float64 = 0.01)
    kept_indices = findall(w -> w >= threshold, gmb.weights)
    
    if isempty(kept_indices)
        # Keep the component with maximum weight
        max_idx = argmax(gmb.weights)
        kept_indices = [max_idx]
    end
    
    new_components = gmb.components[kept_indices]
    new_weights = gmb.weights[kept_indices]
    new_weights ./= sum(new_weights)  # Renormalize
    
    return GaussianMixtureBelief(new_components, new_weights)
end



function MergeComponents(gmb::GaussianMixtureBelief; max_components::Int = 5)
    if length(gmb.components) <= max_components
        return gmb
    end
    
    components = copy(gmb.components)
    weights = copy(gmb.weights)
    
    while length(components) > max_components
        # Find pair with minimum KL divergence
        min_cost = Inf
        merge_i, merge_j = 1, 2
        
        for i in 1:(length(components)-1)
            for j in (i+1):length(components)
                cost = KLDivergence(components[i], components[j], weights[i], weights[j])
                if cost < min_cost
                    min_cost = cost
                    merge_i, merge_j = i, j
                end
            end
        end
        
        # Merge the two closest components
        merged_component, merged_weight = MergeTwoGaussians(
            components[merge_i], components[merge_j], 
            weights[merge_i], weights[merge_j]
        )
        
        # Remove old components and add merged one
        deleteat!(components, max(merge_i, merge_j))
        deleteat!(components, min(merge_i, merge_j))
        deleteat!(weights, max(merge_i, merge_j))
        deleteat!(weights, min(merge_i, merge_j))
        
        push!(components, merged_component)
        push!(weights, merged_weight)
    end
    
    return GaussianMixtureBelief(components, weights)
end

function MergeTwoGaussians(g1::FullNormal, g2::FullNormal, w1::Float64, w2::Float64)
    w_total = w1 + w2
    
    # Merged mean (weighted average)
    Œº_merged = (w1 * g1.Œº + w2 * g2.Œº) / w_total
    
    # Merged covariance (moment matching)
    Œ£_merged = (w1 * (g1.Œ£ + (g1.Œº - Œº_merged) * (g1.Œº - Œº_merged)') + 
                w2 * (g2.Œ£ + (g2.Œº - Œº_merged) * (g2.Œº - Œº_merged)')) / w_total
    
    return MvNormal(Œº_merged, Œ£_merged), w_total
end


function SplitComponent(component::FullNormal, num_splits::Int = 4)
    # Split a single Gaussian into multiple components for better nonlinearity handling
    Œº = component.Œº
    Œ£ = component.Œ£
    
    components = FullNormal[]
    weights = Float64[]
    
    # Create sigma points around the mean
    n = length(Œº)
    Œª = 2.0  # Scaling parameter
    
    # Central component
    push!(components, component)
    push!(weights, 0.5)
    
    # Surrounding components
    for i in 1:n
        offset = sqrt((n + Œª) * Œ£[i,i]) * [j == i ? 1.0 : 0.0 for j in 1:n]
        
        # Positive direction
        Œº_pos = Œº + offset
        Œ£_split = Œ£ / (num_splits / 2)  # Reduce covariance
        push!(components, MvNormal(Œº_pos, Œ£_split))
        push!(weights, 0.25 / n)
        
        # Negative direction  
        Œº_neg = Œº - offset
        push!(components, MvNormal(Œº_neg, Œ£_split))
        push!(weights, 0.25 / n)
    end
    
    # Normalize weights
    weights ./= sum(weights)
    
    return GaussianMixtureBelief(components, weights)
end


# Add these visualization functions to GMFFunctions.jl

function PlotGMFResults(ùí´::POMDPscenario, œÑ, œÑbGMF)
    fig = plot(size=(800,600))

    # beacons
    scatter!(fig, ùí´.beacons[:,1], ùí´.beacons[:,2];
             m=:utriangle, c=:red, ms=8, label="Beacons")

    # ground truth as blue dots
    scatter!(fig, getindex.(œÑ,1), getindex.(œÑ,2);
             m=:circle, c=:blue, ms=4, label="Ground truth")

    T = length(œÑbGMF)
    for t in 1:T
        gmb = œÑbGMF[t]
        fade = 0.15 + 0.8*t/T            # early ‚Üí very light, final ‚Üí darker

        # 2.a ‚ÄÉall components (faded)
        for (i,comp) in enumerate(gmb.components)
            scatter!(fig, [comp.Œº[1]], [comp.Œº[2]];
                     m=:circle, ms=max(3,35*gmb.weights[i]),
                     c=:green, alpha=0.30*fade,
                     label = (t==1 && i==1 ? "Component means" : ""))

            ex, ey = drawCovarianceEllipse(comp.Œº, comp.Œ£, 2)
            plot!(fig, ex, ey; c=:green, alpha=0.15*fade, lw=1, label="")
        end

        # 2.b ‚ÄÉweighted posterior (mixture moments)
        ŒºÃÑ, Œ£ÃÑ = mixtureMoments(gmb)
        scatter!(fig, [ŒºÃÑ[1]], [ŒºÃÑ[2]];
                 m=:star5, ms=8,  c=:black, alpha=fade,
                 label = (t==1 ? "Posterior mean" : ""))

        exM, eyM = drawCovarianceEllipse(ŒºÃÑ, Œ£ÃÑ, 2)
        plot!(fig, exM, eyM; c=:black, alpha=0.15*fade, lw=1.5,
              linestyle=:dash, label=(t==1 ? "Posterior 2œÉ" : ""))
    end

    title!(fig, "GMF Posterior Evolution")
    xlabel!(fig, "X position")
    ylabel!(fig, "Y position")
    savefig(fig, "Results/GMF_MixtureComponents.pdf")
    return fig
end





function SaveGMFEvolution(œÑbGMF, T)
    # Save detailed evolution for analysis
    open("Results/GMF_Evolution.txt", "w") do f
        for t in 1:T
            gmb = œÑbGMF[t]
            println(f, "Time step $t:")
            println(f, "  Number of components: $(length(gmb.components))")
            for (i, (comp, weight)) in enumerate(zip(gmb.components, gmb.weights))
                println(f, "  Component $i: weight=$(round(weight, digits=4)), mean=$(comp.Œº)")
            end
            println(f, "")
        end
    end
end



# GMF-specific versions of the calculation functions
function CalculateDeadReckoningBeliefGMF(ùí´::POMDPscenario, b0GMF::GaussianMixtureBelief, ak_array, T)
    œÑbdr = Array{GaussianMixtureBelief}(undef, T)
    œÑbdr[1] = b0GMF
    
    for t in 2:T
        # FIX: Use specific action for each time step
        œÑbdr[t] = PropagateBelief(œÑbdr[t-1], ùí´, ak_array[t-1])
        
        # Apply pruning and merging to manage complexity
        œÑbdr[t] = PruneComponents(œÑbdr[t])
        œÑbdr[t] = MergeComponents(œÑbdr[t], max_components=5)
    end
    
    return œÑbdr
end

function CalculatePosteriorBeliefGMF(ùí´::POMDPscenario, b0GMF::GaussianMixtureBelief,
                                     ak, T, œÑobs)

    œÑb = Vector{GaussianMixtureBelief}(undef, T)
    œÑb[1] = b0GMF

    Œµsplit   = 10.0          # non-linearity threshold
    Nmax     = 8           # max components after merge
    wprune   = 1e-3         # prune threshold

    for t in 2:T
        # ---------------- growth (splitting) --------------------------
        gmb_grow = GaussianMixtureBelief([], Float64[])
        for (comp,w) in zip(œÑb[t-1].components, œÑb[t-1].weights)
            if (œÑobs[t] !== nothing) && (nonlinScore(comp, œÑobs[t].obs, ùí´) > Œµsplit)
                parts, loc_w = splitComponent(comp)
                append!(gmb_grow.components, parts)
                append!(gmb_grow.weights,    w .* loc_w)
            else
                push!(gmb_grow.components, comp)
                push!(gmb_grow.weights,    w)
            end
        end
        gmb_grow.weights ./= sum(gmb_grow.weights)   # normalise

        # -------------- predict & (optional) update -------------------
        if œÑobs[t] === nothing
            gmb_pred = PropagateBelief(gmb_grow, ùí´, ak[t-1])
            gmb_upd  = gmb_pred                         # no observation
        else
            obs_corr = œÑobs[t].obs .+ ùí´.beacons[œÑobs[t].index,:]
            gmb_upd  = PropagateUpdateBelief(gmb_grow, ùí´, ak[t-1], obs_corr)
        end

        # -------------- prune & merge (shrink) ------------------------
        gmb_upd  = PruneComponents(gmb_upd;  threshold = wprune)
        gmb_upd  = MergeComponents(gmb_upd; max_components = Nmax)

        œÑb[t] = gmb_upd
    end
    return œÑb
end


# Comparison function to analyze filter performance
function PrintFilterComparison(œÑ_true, œÑbG, œÑbP, œÑbGMF, T)
    println("\n=== FILTER PERFORMANCE COMPARISON ===")
    
    for t in 1:T
        true_pos = œÑ_true[t]
        
        # Gaussian filter error
        gauss_error = norm(œÑbG[t].Œº - true_pos)
        
        # Particle filter error (using weighted mean)
        pf_mean = sum(œÑbP[t].weights[i] * œÑbP[t].particles[i] for i in 1:length(œÑbP[t].particles))
        pf_error = norm(pf_mean - true_pos)
        
        # GMF error (using weighted mean of mixture)
        gmf_mean = sum(œÑbGMF[t].weights[i] * œÑbGMF[t].components[i].Œº for i in 1:length(œÑbGMF[t].components))
        gmf_error = norm(gmf_mean - true_pos)
        
        println("Step $t: G_err=$(round(gauss_error,digits=3)), PF_err=$(round(pf_error,digits=3)), GMF_err=$(round(gmf_error,digits=3))")
    end
end


# Add this function to GMFFunctions.jl
function KLDivergence(g1::FullNormal, g2::FullNormal, w1::Float64, w2::Float64)
    # Simplified KL divergence approximation for merging decision
    # Using Mahalanobis distance as a proxy
    Œº_diff = g1.Œº - g2.Œº
    Œ£_avg = (w1 * g1.Œ£ + w2 * g2.Œ£) / (w1 + w2)
    
    try
        distance = sqrt(Œº_diff' * inv(Œ£_avg) * Œº_diff)
        return distance
    catch
        # Fallback if matrix inversion fails
        return norm(Œº_diff)
    end
end


function CreateSigmaPointGMF(Œº0, Œ£0, num_components)
    # Simple implementation - distribute components around the mean
    components = FullNormal[]
    weights = Float64[]
    
    weight_per_component = 1.0 / num_components
    
    for i in 1:num_components
        # Create slight variations around the mean
        angle = 2œÄ * (i-1) / num_components
        radius = 1.0
        offset = radius * [cos(angle), sin(angle)]
        
        Œº_comp = Œº0 + offset
        Œ£_comp = Œ£0 / num_components
        
        push!(components, MvNormal(Œº_comp, Œ£_comp))
        push!(weights, weight_per_component)
    end
    
    return components, weights
end
